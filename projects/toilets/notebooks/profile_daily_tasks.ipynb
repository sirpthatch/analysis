{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Tasks Data Profiling\n",
    "\n",
    "This notebook provides basic profiling of the daily tasks data in `data/daily_tasks/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Data Profiling Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data files\n",
    "data_dir = Path('../data/daily_tasks')\n",
    "data_files = list(data_dir.glob('*.csv'))\n",
    "\n",
    "print(f\"Found {len(data_files)} data files:\")\n",
    "for file in sorted(data_files):\n",
    "    size_mb = file.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {file.name}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample file to understand structure\n",
    "sample_file = data_dir / '2024.csv'\n",
    "print(f\"Loading sample from {sample_file.name}...\")\n",
    "\n",
    "# Read a small sample first\n",
    "sample_df = pd.read_csv(sample_file, nrows=1000)\n",
    "print(f\"Sample shape: {sample_df.shape}\")\n",
    "print(f\"\\nColumns ({len(sample_df.columns)}):\")\n",
    "print(sample_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info about the sample data\n",
    "print(\"Data Types:\")\n",
    "print(sample_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"Missing Values:\")\n",
    "missing_counts = sample_df.isnull().sum()\n",
    "missing_pct = (missing_counts / len(sample_df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing %', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values for key categorical columns\n",
    "categorical_cols = ['district', 'sector', 'activity', 'animal_waste', 'broken_glass', \n",
    "                   'dumping', 'graffiti', 'medical_waste', 'fixed_post']\n",
    "\n",
    "print(\"Unique values for key categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    if col in sample_df.columns:\n",
    "        unique_count = sample_df[col].nunique()\n",
    "        print(f\"{col}: {unique_count} unique values\")\n",
    "        if unique_count <= 10:\n",
    "            print(f\"  Values: {sample_df[col].unique()}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric columns analysis\n",
    "numeric_cols = sample_df.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Numeric columns: {list(numeric_cols)}\")\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    print(\"\\nNumeric columns summary:\")\n",
    "    sample_df[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date analysis\n",
    "date_cols = ['date_worked', 'start_time', 'end_time']\n",
    "for col in date_cols:\n",
    "    if col in sample_df.columns:\n",
    "        print(f\"\\n{col} examples:\")\n",
    "        print(sample_df[col].head())\n",
    "        \n",
    "        # Try to parse dates\n",
    "        try:\n",
    "            parsed_dates = pd.to_datetime(sample_df[col])\n",
    "            print(f\"Date range: {parsed_dates.min()} to {parsed_dates.max()}\")\n",
    "        except:\n",
    "            print(\"Could not parse as datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full datasets to get overview statistics\n",
    "file_stats = []\n",
    "\n",
    "for file in sorted(data_files):\n",
    "    print(f\"Processing {file.name}...\")\n",
    "    \n",
    "    # Get basic stats without loading full file into memory\n",
    "    chunk_size = 10000\n",
    "    total_rows = 0\n",
    "    \n",
    "    for chunk in pd.read_csv(file, chunksize=chunk_size):\n",
    "        total_rows += len(chunk)\n",
    "    \n",
    "    file_stats.append({\n",
    "        'file': file.name,\n",
    "        'year': file.stem,\n",
    "        'total_rows': total_rows,\n",
    "        'size_mb': file.stat().st_size / (1024 * 1024)\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(file_stats)\n",
    "print(\"\\nFile Statistics:\")\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize file statistics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Rows by year\n",
    "ax1.bar(stats_df['year'], stats_df['total_rows'])\n",
    "ax1.set_title('Total Rows by Year')\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Number of Rows')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# File size by year\n",
    "ax2.bar(stats_df['year'], stats_df['size_mb'])\n",
    "ax2.set_title('File Size by Year (MB)')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_ylabel('Size (MB)')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total records across all files: {stats_df['total_rows'].sum():,}\")\n",
    "print(f\"Total data size: {stats_df['size_mb'].sum():.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity distribution analysis (from sample)\n",
    "if 'activity' in sample_df.columns:\n",
    "    activity_counts = sample_df['activity'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    activity_counts.plot(kind='bar')\n",
    "    plt.title('Activity Distribution (Sample)')\n",
    "    plt.xlabel('Activity Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Activity distribution:\")\n",
    "    print(activity_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borough/District analysis (from sample)\n",
    "if 'district' in sample_df.columns:\n",
    "    district_counts = sample_df['district'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    district_counts.head(20).plot(kind='bar')\n",
    "    plt.title('Top 20 Districts by Task Count (Sample)')\n",
    "    plt.xlabel('District')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Total unique districts: {sample_df['district'].nunique()}\")\n",
    "    print(\"\\nTop 10 districts:\")\n",
    "    print(district_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This profiling provides an overview of the daily tasks data structure, including:\n",
    "\n",
    "- **Data Volume**: Multiple years of data with hundreds of thousands of records\n",
    "- **Key Columns**: Activity types, districts, sectors, timestamps, and various waste/maintenance flags\n",
    "- **Data Quality**: Missing value analysis and data type verification\n",
    "- **Temporal Coverage**: Multi-year dataset spanning 2022-2025\n",
    "- **Geographic Coverage**: Multiple districts and sectors across NYC\n",
    "\n",
    "The data appears to be NYC Department of Sanitation daily task records with detailed location, timing, and activity information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}