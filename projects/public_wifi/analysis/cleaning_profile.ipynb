{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Park Cleaning Records Data Profile\n",
    "\n",
    "This notebook provides a comprehensive profile of the daily tasks park cleaning records dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/daily_tasks_park_cleaning_records_20250923.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"=== DATASET INFO ===\")\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names and types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMemory usage: {df.memory_usage().sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"=== FIRST 5 ROWS ===\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing Count': missing_data.values,\n",
    "    'Missing Percentage': missing_percent.values\n",
    "})\n",
    "display(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date and time analysis\n",
    "print(\"=== TEMPORAL ANALYSIS ===\")\n",
    "\n",
    "# Convert date/time columns\n",
    "if 'date_worked' in df.columns:\n",
    "    df['date_worked'] = pd.to_datetime(df['date_worked'])\n",
    "    print(f\"Date range: {df['date_worked'].min()} to {df['date_worked'].max()}\")\n",
    "    print(f\"Unique dates: {df['date_worked'].nunique()}\")\n",
    "\n",
    "if 'start_time' in df.columns:\n",
    "    df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "    df['end_time'] = pd.to_datetime(df['end_time'])\n",
    "    \n",
    "    # Extract hour for shift analysis\n",
    "    df['start_hour'] = df['start_time'].dt.hour\n",
    "    print(f\"\\nWorking hours range: {df['start_hour'].min()}:00 to {df['start_hour'].max()}:00\")\n",
    "    \n",
    "    # Calculate duration\n",
    "    df['duration_hours'] = (df['end_time'] - df['start_time']).dt.total_seconds() / 3600\n",
    "    print(f\"Task duration - Mean: {df['duration_hours'].mean():.2f}h, Range: {df['duration_hours'].min():.2f}h - {df['duration_hours'].max():.2f}h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity type analysis\n",
    "print(\"=== ACTIVITY ANALYSIS ===\")\n",
    "\n",
    "if 'activity' in df.columns:\n",
    "    activity_counts = df['activity'].value_counts()\n",
    "    print(\"Tasks by activity type:\")\n",
    "    display(activity_counts)\n",
    "    \n",
    "    print(f\"\\nTotal unique activities: {df['activity'].nunique()}\")\n",
    "    \n",
    "    # Work vs non-work activities\n",
    "    work_pct = (df['activity'] == 'Work').sum() / len(df) * 100\n",
    "    print(f\"Work activities: {work_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Waste and maintenance issues analysis\n",
    "print(\"=== WASTE & MAINTENANCE ISSUES ===\")\n",
    "\n",
    "waste_columns = ['animal_waste', 'broken_glass', 'dumping', 'graffiti', 'medical_waste']\n",
    "boolean_columns = ['animal_waste', 'broken_glass', 'dumping', 'graffiti', 'medical_waste']\n",
    "\n",
    "# Convert Yes/No to boolean for analysis\n",
    "for col in boolean_columns:\n",
    "    if col in df.columns:\n",
    "        df[f'{col}_bool'] = df[col] == 'Yes'\n",
    "\n",
    "# Count issues\n",
    "issue_stats = {}\n",
    "for col in waste_columns:\n",
    "    if col in df.columns:\n",
    "        yes_count = (df[col] == 'Yes').sum()\n",
    "        yes_pct = yes_count / len(df) * 100\n",
    "        issue_stats[col] = {'count': yes_count, 'percentage': yes_pct}\n",
    "        print(f\"{col}: {yes_count} occurrences ({yes_pct:.1f}%)\")\n",
    "\n",
    "# Most common issue\n",
    "if issue_stats:\n",
    "    most_common = max(issue_stats.items(), key=lambda x: x[1]['count'])\n",
    "    print(f\"\\nMost common issue: {most_common[0]} ({most_common[1]['count']} occurrences)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic and Organizational Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# District and sector analysis\n",
    "print(\"=== GEOGRAPHIC ANALYSIS ===\")\n",
    "\n",
    "if 'district' in df.columns:\n",
    "    district_counts = df['district'].value_counts()\n",
    "    print(\"Tasks by district:\")\n",
    "    display(district_counts)\n",
    "\n",
    "if 'sector_name' in df.columns:\n",
    "    sector_counts = df['sector_name'].value_counts().head(10)\n",
    "    print(\"\\nTop 10 sectors by task count:\")\n",
    "    display(sector_counts)\n",
    "\n",
    "# Vehicle utilization\n",
    "if 'vehicle_number' in df.columns:\n",
    "    vehicle_stats = df['vehicle_number'].value_counts()\n",
    "    print(f\"\\nVehicle utilization:\")\n",
    "    print(f\"Total vehicles used: {df['vehicle_number'].nunique()}\")\n",
    "    print(f\"Most used vehicle: {vehicle_stats.index[0]} ({vehicle_stats.iloc[0]} tasks)\")\n",
    "    print(f\"Average tasks per vehicle: {vehicle_stats.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workforce Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crew and workforce analysis\n",
    "print(\"=== WORKFORCE ANALYSIS ===\")\n",
    "\n",
    "workforce_cols = ['napsw', 'ncpw', 'ncsa', 'npop', 'nnpw', 'ncrew']\n",
    "workforce_data = {}\n",
    "\n",
    "for col in workforce_cols:\n",
    "    if col in df.columns:\n",
    "        workforce_data[col] = {\n",
    "            'mean': df[col].mean(),\n",
    "            'max': df[col].max(),\n",
    "            'total': df[col].sum()\n",
    "        }\n",
    "        print(f\"{col} - Mean: {df[col].mean():.2f}, Max: {df[col].max()}, Total: {df[col].sum()}\")\n",
    "\n",
    "# Fixed post analysis\n",
    "if 'fixed_post' in df.columns:\n",
    "    fixed_post_counts = df['fixed_post'].value_counts()\n",
    "    print(f\"\\nFixed post assignments:\")\n",
    "    display(fixed_post_counts)\n",
    "\n",
    "# Hours analysis\n",
    "if 'nhours' in df.columns:\n",
    "    print(f\"\\nHours statistics:\")\n",
    "    print(f\"Total hours: {df['nhours'].sum():.2f}\")\n",
    "    print(f\"Average hours per task: {df['nhours'].mean():.2f}\")\n",
    "    print(f\"Hour range: {df['nhours'].min():.2f} - {df['nhours'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for data quality issues\n",
    "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
    "\n",
    "# Duplicate analysis\n",
    "print(f\"Total duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"Duplicate daily_task_ids: {df['daily_task_id'].duplicated().sum()}\")\n",
    "\n",
    "# Time consistency checks\n",
    "if 'start_time' in df.columns and 'end_time' in df.columns:\n",
    "    negative_duration = (df['end_time'] < df['start_time']).sum()\n",
    "    print(f\"Tasks with negative duration: {negative_duration}\")\n",
    "    \n",
    "    zero_duration = (df['duration_hours'] == 0).sum()\n",
    "    print(f\"Tasks with zero duration: {zero_duration}\")\n",
    "\n",
    "# Check for impossible values\n",
    "if 'nhours' in df.columns:\n",
    "    impossible_hours = (df['nhours'] > 24).sum()\n",
    "    print(f\"Tasks with >24 hours: {impossible_hours}\")\n",
    "\n",
    "# Missing critical data\n",
    "if 'gispropnum' in df.columns:\n",
    "    missing_locations = df['gispropnum'].isnull().sum()\n",
    "    print(f\"Records missing location data: {missing_locations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# Activity distribution\n",
    "if 'activity' in df.columns:\n",
    "    activity_counts = df['activity'].value_counts()\n",
    "    axes[0, 0].pie(activity_counts.values, labels=activity_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Distribution of Activity Types')\n",
    "\n",
    "# Start hour distribution\n",
    "if 'start_hour' in df.columns:\n",
    "    hour_counts = df['start_hour'].value_counts().sort_index()\n",
    "    axes[0, 1].bar(hour_counts.index, hour_counts.values)\n",
    "    axes[0, 1].set_title('Tasks by Start Hour')\n",
    "    axes[0, 1].set_xlabel('Hour of Day')\n",
    "    axes[0, 1].set_ylabel('Number of Tasks')\n",
    "\n",
    "# Duration distribution\n",
    "if 'duration_hours' in df.columns:\n",
    "    axes[1, 0].hist(df['duration_hours'], bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Task Duration Distribution')\n",
    "    axes[1, 0].set_xlabel('Duration (hours)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Issues frequency\n",
    "if any(col in df.columns for col in waste_columns):\n",
    "    issue_counts = []\n",
    "    issue_names = []\n",
    "    for col in waste_columns:\n",
    "        if col in df.columns:\n",
    "            count = (df[col] == 'Yes').sum()\n",
    "            issue_counts.append(count)\n",
    "            issue_names.append(col.replace('_', ' ').title())\n",
    "    \n",
    "    axes[1, 1].barh(issue_names, issue_counts)\n",
    "    axes[1, 1].set_title('Frequency of Issues')\n",
    "    axes[1, 1].set_xlabel('Number of Occurrences')\n",
    "\n",
    "# District distribution\n",
    "if 'district' in df.columns:\n",
    "    district_counts = df['district'].value_counts()\n",
    "    axes[2, 0].bar(range(len(district_counts)), district_counts.values)\n",
    "    axes[2, 0].set_xticks(range(len(district_counts)))\n",
    "    axes[2, 0].set_xticklabels(district_counts.index, rotation=45)\n",
    "    axes[2, 0].set_title('Tasks by District')\n",
    "    axes[2, 0].set_ylabel('Number of Tasks')\n",
    "\n",
    "# Crew size distribution\n",
    "if 'ncrew' in df.columns:\n",
    "    crew_counts = df['ncrew'].value_counts().sort_index()\n",
    "    axes[2, 1].bar(crew_counts.index, crew_counts.values)\n",
    "    axes[2, 1].set_title('Distribution of Crew Sizes')\n",
    "    axes[2, 1].set_xlabel('Number of Crew Members')\n",
    "    axes[2, 1].set_ylabel('Number of Tasks')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "if 'date_worked' in df.columns:\n",
    "    print(\"=== TIME SERIES ANALYSIS ===\")\n",
    "    \n",
    "    # Daily task counts\n",
    "    daily_counts = df['date_worked'].value_counts().sort_index()\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(daily_counts.index, daily_counts.values, marker='o')\n",
    "    plt.title('Daily Task Volume Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Number of Tasks')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Average daily tasks: {daily_counts.mean():.2f}\")\n",
    "    print(f\"Peak day: {daily_counts.idxmax()} ({daily_counts.max()} tasks)\")\n",
    "    print(f\"Lowest day: {daily_counts.idxmin()} ({daily_counts.min()} tasks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Analysis & Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation analysis\n",
    "print(\"=== CROSS-TABULATION ANALYSIS ===\")\n",
    "\n",
    "if 'district' in df.columns and 'activity' in df.columns:\n",
    "    print(\"\\nDistrict vs Activity:\")\n",
    "    crosstab1 = pd.crosstab(df['district'], df['activity'], margins=True)\n",
    "    display(crosstab1)\n",
    "\n",
    "# Issue correlation analysis\n",
    "if all(col in df.columns for col in waste_columns):\n",
    "    print(\"\\n=== ISSUE CORRELATION MATRIX ===\")\n",
    "    \n",
    "    # Create boolean matrix for issues\n",
    "    issue_matrix = df[waste_columns].replace({'Yes': 1, 'No': 0})\n",
    "    correlation_matrix = issue_matrix.corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, fmt='.2f')\n",
    "    plt.title('Correlation Matrix of Waste/Maintenance Issues')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency analysis\n",
    "print(\"=== EFFICIENCY ANALYSIS ===\")\n",
    "\n",
    "if 'nhours' in df.columns and 'ncrew' in df.columns:\n",
    "    # Productivity metrics\n",
    "    df['crew_hours'] = df['nhours'] * df['ncrew']\n",
    "    \n",
    "    efficiency_by_district = df.groupby('district').agg({\n",
    "        'crew_hours': 'sum',\n",
    "        'row_id': 'count',\n",
    "        'nhours': 'mean'\n",
    "    }).rename(columns={'row_id': 'total_tasks'})\n",
    "    \n",
    "    efficiency_by_district['hours_per_task'] = efficiency_by_district['crew_hours'] / efficiency_by_district['total_tasks']\n",
    "    \n",
    "    print(\"Efficiency by district:\")\n",
    "    display(efficiency_by_district.sort_values('hours_per_task'))\n",
    "\n",
    "# Peak hours analysis\n",
    "if 'start_hour' in df.columns:\n",
    "    peak_hours = df.groupby('start_hour').agg({\n",
    "        'row_id': 'count',\n",
    "        'nhours': 'sum'\n",
    "    }).rename(columns={'row_id': 'task_count', 'nhours': 'total_hours'})\n",
    "    \n",
    "    print(\"\\nActivity by hour:\")\n",
    "    display(peak_hours.sort_values('task_count', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA QUALITY SUMMARY & RECOMMENDATIONS ===\")\n",
    "\n",
    "print(\"\\n1. COMPLETENESS:\")\n",
    "critical_cols = ['gispropnum', 'date_worked', 'start_time', 'end_time', 'activity']\n",
    "for col in critical_cols:\n",
    "    if col in df.columns:\n",
    "        missing_pct = (df[col].isnull().sum() / len(df)) * 100\n",
    "        if missing_pct > 0:\n",
    "            print(f\"   - {col}: {missing_pct:.1f}% missing values\")\n",
    "\n",
    "print(\"\\n2. TEMPORAL CONSISTENCY:\")\n",
    "if 'start_time' in df.columns and 'end_time' in df.columns:\n",
    "    time_issues = (df['end_time'] <= df['start_time']).sum()\n",
    "    print(f\"   - Tasks with end time before/equal to start time: {time_issues}\")\n",
    "    \n",
    "    if 'nhours' in df.columns:\n",
    "        hour_mismatch = abs(df['duration_hours'] - df['nhours']).sum()\n",
    "        print(f\"   - Potential hour calculation mismatches: {hour_mismatch:.2f}\")\n",
    "\n",
    "print(\"\\n3. LOGICAL CONSISTENCY:\")\n",
    "if 'ncrew' in df.columns:\n",
    "    zero_crew = (df['ncrew'] == 0).sum()\n",
    "    print(f\"   - Tasks with zero crew members: {zero_crew}\")\n",
    "\n",
    "if 'vehicle_number' in df.columns:\n",
    "    missing_vehicles = df['vehicle_number'].isnull().sum()\n",
    "    print(f\"   - Tasks missing vehicle assignment: {missing_vehicles}\")\n",
    "\n",
    "print(\"\\n4. OPERATIONAL INSIGHTS:\")\n",
    "if 'activity' in df.columns:\n",
    "    work_pct = (df['activity'] == 'Work').sum() / len(df) * 100\n",
    "    print(f\"   - Work vs non-work ratio: {work_pct:.1f}% work activities\")\n",
    "\n",
    "if any(col in df.columns for col in waste_columns):\n",
    "    total_issues = sum((df[col] == 'Yes').sum() for col in waste_columns if col in df.columns)\n",
    "    avg_issues_per_task = total_issues / len(df)\n",
    "    print(f\"   - Average issues per task: {avg_issues_per_task:.2f}\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS:\")\n",
    "recommendations = [\n",
    "    \"Implement real-time data validation for start/end times\",\n",
    "    \"Standardize missing value handling for location data\",\n",
    "    \"Add data quality checks for crew and vehicle assignments\",\n",
    "    \"Consider automated duration calculation based on start/end times\",\n",
    "    \"Implement outlier detection for unusual task durations\",\n",
    "    \"Create dashboard for monitoring daily productivity metrics\",\n",
    "    \"Establish data retention policies for historical analysis\",\n",
    "    \"Consider geocoding validation for location accuracy\"\n",
    "]\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"   {i}. {rec}\")\n",
    "\n",
    "print(\"\\n6. KEY METRICS FOR MONITORING:\")\n",
    "metrics = [\n",
    "    \"Daily task completion rates\",\n",
    "    \"Average crew hours per task\",\n",
    "    \"Issue frequency trends\",\n",
    "    \"Vehicle utilization rates\",\n",
    "    \"Sector-wise productivity\",\n",
    "    \"Peak operational hours\"\n",
    "]\n",
    "\n",
    "for i, metric in enumerate(metrics, 1):\n",
    "    print(f\"   {i}. {metric}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}