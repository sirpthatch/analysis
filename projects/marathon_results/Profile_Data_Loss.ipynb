{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f00f543b",
   "metadata": {},
   "source": [
    "# Filtering Data Loss\n",
    "This notebook profiles how we loose data as we join race information and weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8eb4000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756d5f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Starting Rows: 12,656,862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "RAW_RACE_RESULTS_DIR = \"data/race_results\"\n",
    "\n",
    "total_rows = 0\n",
    "\n",
    "for race_file in os.listdir(RAW_RACE_RESULTS_DIR):\n",
    "    filepath = os.path.join(RAW_RACE_RESULTS_DIR, race_file)\n",
    "    df = pd.read_parquet(filepath)\n",
    "    \n",
    "    total_rows += df.shape[0]\n",
    "\n",
    "total_raw_race_result_rows = total_rows\n",
    "print(f\"Total Starting Rows: {total_raw_race_result_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5562e12",
   "metadata": {},
   "source": [
    "# Step 1: Extract Race Records from Race Results\n",
    "In this step we extracted normalized race records from the raw race results files.  We filtered some records that did not have sufficient metadata for the downstream analysis.  This is done in the Data_Profile_Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635b42d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Race record rows: 10,478,198\n",
      " dropped 2,178,664 rows, 17.21%\n"
     ]
    }
   ],
   "source": [
    "RACE_RECORDS_DIR = \"data/race_records\"\n",
    "total_rows = 0\n",
    "\n",
    "for race_file in os.listdir(RACE_RECORDS_DIR):\n",
    "    filepath = os.path.join(RACE_RECORDS_DIR, race_file)\n",
    "    df = pd.read_parquet(filepath)\n",
    "    \n",
    "    total_rows += df.shape[0]\n",
    "\n",
    "total_race_records_rows = total_rows\n",
    "dropped_rows = total_raw_race_result_rows-total_race_records_rows\n",
    "print(f\"Race record rows: {total_race_records_rows:,}\")\n",
    "print(f\" dropped {dropped_rows:,} rows, {dropped_rows/total_raw_race_result_rows*100.0:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f1cb6",
   "metadata": {},
   "source": [
    "# Step 2: Filter Race Records to ones that have a non null city and state\n",
    "\n",
    "This is done in the process_racedata.py module, and is necessary because we need the city and state to do our downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6426b678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final race record rows: 7,411,671\n",
      " dropped 3,066,527 rows, 29.27%\n"
     ]
    }
   ],
   "source": [
    "RACE_FINAL_FILE = \"data/race_final/global/data.csv\"\n",
    "df = pd.read_csv(RACE_FINAL_FILE)\n",
    "\n",
    "total_race_final_rows = df.shape[0]\n",
    "dropped_rows = total_race_records_rows - total_race_final_rows\n",
    "print(f\"Final race record rows: {total_race_final_rows:,}\")\n",
    "print(f\" dropped {dropped_rows:,} rows, {dropped_rows/total_race_records_rows*100.0:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cea86a",
   "metadata": {},
   "source": [
    "# Step 3: Construct the featurized weather data frame\n",
    "In this step we join the weather data with the race results.  This is done in the Map_Weather_Features.ipynb file, and relies on a weather forceast dataset that is built in the Map_Cities.ipynb notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855b641",
   "metadata": {},
   "source": [
    "## Step 3(a): Filter the race records to ones that are after 2016\n",
    "We only pull weather data after 2015 to simplify things, so races that are run in 2015 and before will not have enough information for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b702151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total in date range race record rows: 2,170,201\n",
      " dropped 5,241,470 rows, 70.72%\n"
     ]
    }
   ],
   "source": [
    "RACE_FINAL_FILE = \"data/race_final/global/data.csv\"\n",
    "df = pd.read_csv(RACE_FINAL_FILE)\n",
    "\n",
    "def parse_date_column(col:str) -> datetime:\n",
    "    (month, day, year) = col.split(\"_\")\n",
    "    return datetime(int(\"20\"+year), int(month), int(day))\n",
    "\n",
    "df[\"date\"] = df[\"date\"].apply(parse_date_column)\n",
    "\n",
    "EARLIEST_WEATHER = datetime(2015,1,1)\n",
    "race_df_filtered_date = df[df[\"date\"]> EARLIEST_WEATHER + timedelta(days=365)]\n",
    "\n",
    "total_inrange_race_final_rows = race_df_filtered_date.shape[0]\n",
    "dropped_rows = total_race_final_rows - total_inrange_race_final_rows\n",
    "print(f\"Total in date range race record rows: {total_inrange_race_final_rows:,}\")\n",
    "print(f\" dropped {dropped_rows:,} rows, {dropped_rows/total_race_final_rows*100.0:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2fb962",
   "metadata": {},
   "source": [
    "## Step 3(b): Filter the race records to ones that have cities that join with the weather data\n",
    "We only pulled weather data for a subset of the cities, and so we can only use race records for runners that are from those cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f247bd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total location filtered race record rows: 358,982\n",
      " dropped 1,811,219 rows, 83.46%\n"
     ]
    }
   ],
   "source": [
    "WEATHER_DATA = \"data/weather_data.csv\"\n",
    "\n",
    "weather_data = pd.read_csv(WEATHER_DATA, parse_dates=['date'])\n",
    "available_weather_cities = set(weather_data.groupby([\"city\",\"state\"]).groups)\n",
    "\n",
    "# Create a set of lowercase (city, state) tuples for fast lookup\n",
    "available_weather_cities_lower = {(city.lower(), state.lower()) for (city, state) in available_weather_cities}\n",
    "\n",
    "# Filter to only records with matching weather data\n",
    "race_df_filtered_loc = race_df_filtered_date[\n",
    "    race_df_filtered_date.apply(\n",
    "        lambda row: (row['city'], row['state']) in available_weather_cities_lower, \n",
    "        axis=1\n",
    "    )\n",
    "]\n",
    "\n",
    "unavailable_cities = race_df_filtered_date[~race_df_filtered_date.apply(\n",
    "        lambda row: (row['city'], row['state']) in available_weather_cities_lower, \n",
    "        axis=1\n",
    "    )].groupby([\"city\",\"state\"]).groups\n",
    "\n",
    "total_loc_race_final_rows = race_df_filtered_loc.shape[0]\n",
    "dropped_rows = total_inrange_race_final_rows - total_loc_race_final_rows\n",
    "print(f\"Total location filtered race record rows: {total_loc_race_final_rows:,}\")\n",
    "print(f\" dropped {dropped_rows:,} rows, {dropped_rows/total_inrange_race_final_rows*100.0:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d531e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cincinnati</td>\n",
       "      <td>oh</td>\n",
       "      <td>12053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dallas</td>\n",
       "      <td>tx</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pittsburgh</td>\n",
       "      <td>pa</td>\n",
       "      <td>9681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boston</td>\n",
       "      <td>ma</td>\n",
       "      <td>9136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atlanta</td>\n",
       "      <td>ga</td>\n",
       "      <td>8846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>charlotte</td>\n",
       "      <td>nc</td>\n",
       "      <td>8391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>columbus</td>\n",
       "      <td>oh</td>\n",
       "      <td>7602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>miami</td>\n",
       "      <td>fl</td>\n",
       "      <td>7031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>orlando</td>\n",
       "      <td>fl</td>\n",
       "      <td>6714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>san jose</td>\n",
       "      <td>ca</td>\n",
       "      <td>6399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>raleigh</td>\n",
       "      <td>nc</td>\n",
       "      <td>5969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sacramento</td>\n",
       "      <td>ca</td>\n",
       "      <td>5879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>alexandria</td>\n",
       "      <td>va</td>\n",
       "      <td>5420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>calgary</td>\n",
       "      <td>ab</td>\n",
       "      <td>5376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>louisville</td>\n",
       "      <td>ky</td>\n",
       "      <td>5214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>colorado springs</td>\n",
       "      <td>co</td>\n",
       "      <td>5141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>long beach</td>\n",
       "      <td>ca</td>\n",
       "      <td>5111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>salt lake city</td>\n",
       "      <td>ut</td>\n",
       "      <td>4846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>indianapolis</td>\n",
       "      <td>in</td>\n",
       "      <td>4802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>saint louis</td>\n",
       "      <td>mo</td>\n",
       "      <td>4566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city state  count\n",
       "0         cincinnati    oh  12053\n",
       "1             dallas    tx   9999\n",
       "2         pittsburgh    pa   9681\n",
       "3             boston    ma   9136\n",
       "4            atlanta    ga   8846\n",
       "5          charlotte    nc   8391\n",
       "6           columbus    oh   7602\n",
       "7              miami    fl   7031\n",
       "8            orlando    fl   6714\n",
       "9           san jose    ca   6399\n",
       "10           raleigh    nc   5969\n",
       "11        sacramento    ca   5879\n",
       "12        alexandria    va   5420\n",
       "13           calgary    ab   5376\n",
       "14        louisville    ky   5214\n",
       "15  colorado springs    co   5141\n",
       "16        long beach    ca   5111\n",
       "17    salt lake city    ut   4846\n",
       "18      indianapolis    in   4802\n",
       "19       saint louis    mo   4566"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_cities = race_df_filtered_date[~race_df_filtered_date.apply(\n",
    "        lambda row: (row['city'], row['state']) in available_weather_cities_lower, \n",
    "        axis=1\n",
    "    )].groupby([\"city\",\"state\"]).size().sort_values(ascending=False).reset_index(name='count')\n",
    "\n",
    "missing_cities.head(20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
