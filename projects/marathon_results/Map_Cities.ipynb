{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fa810d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cities_df = pd.read_csv('data/uscities.csv')  # from simplemaps\n",
    "race_cities_df= pd.read_csv('data/top_training_cities_80pct.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "348e6c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new york</td>\n",
       "      <td>NY</td>\n",
       "      <td>40.6943</td>\n",
       "      <td>-73.9249</td>\n",
       "      <td>ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>los angeles</td>\n",
       "      <td>CA</td>\n",
       "      <td>34.1141</td>\n",
       "      <td>-118.4068</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>41.8375</td>\n",
       "      <td>-87.6866</td>\n",
       "      <td>il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>miami</td>\n",
       "      <td>FL</td>\n",
       "      <td>25.7840</td>\n",
       "      <td>-80.2101</td>\n",
       "      <td>fl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>houston</td>\n",
       "      <td>TX</td>\n",
       "      <td>29.7860</td>\n",
       "      <td>-95.3885</td>\n",
       "      <td>tx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city state_id      lat       lng state\n",
       "0     new york       NY  40.6943  -73.9249    ny\n",
       "1  los angeles       CA  34.1141 -118.4068    ca\n",
       "2      chicago       IL  41.8375  -87.6866    il\n",
       "3        miami       FL  25.7840  -80.2101    fl\n",
       "4      houston       TX  29.7860  -95.3885    tx"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_df_p = cities_df[['city','state_id','lat','lng']]\n",
    "cities_df_p['city'] = cities_df_p['city'].apply(str.lower)\n",
    "cities_df_p['state'] = cities_df_p['state_id'].apply(str.lower)\n",
    "cities_df_p.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7927a167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>runner_count</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>los angeles</td>\n",
       "      <td>ca</td>\n",
       "      <td>121020</td>\n",
       "      <td>34.1141</td>\n",
       "      <td>-118.4068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chicago</td>\n",
       "      <td>il</td>\n",
       "      <td>120839</td>\n",
       "      <td>41.8375</td>\n",
       "      <td>-87.6866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york</td>\n",
       "      <td>ny</td>\n",
       "      <td>96250</td>\n",
       "      <td>40.6943</td>\n",
       "      <td>-73.9249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>houston</td>\n",
       "      <td>tx</td>\n",
       "      <td>59373</td>\n",
       "      <td>29.7860</td>\n",
       "      <td>-95.3885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>san diego</td>\n",
       "      <td>ca</td>\n",
       "      <td>57024</td>\n",
       "      <td>32.8313</td>\n",
       "      <td>-117.1222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city state  runner_count      lat       lng\n",
       "0  los angeles    ca        121020  34.1141 -118.4068\n",
       "1      chicago    il        120839  41.8375  -87.6866\n",
       "2     new york    ny         96250  40.6943  -73.9249\n",
       "3      houston    tx         59373  29.7860  -95.3885\n",
       "4    san diego    ca         57024  32.8313 -117.1222"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = race_cities_df.merge(cities_df_p[['city', 'state', 'lat', 'lng']], \n",
    "                          left_on=['city', 'state'], \n",
    "                          right_on=['city', 'state'])\n",
    "merged.to_csv('data/mapped_cities.csv', index=False)\n",
    "merged.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b47ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped: 5934659\n",
      "Unmapped: 691650\n",
      "Coverage:       89.56%\n"
     ]
    }
   ],
   "source": [
    "missing_cities = race_cities_df[~race_cities_df.city.isin(merged.city)]\n",
    "\n",
    "mapped_amount = race_cities_df.runner_count.sum() / (missing_cities.runner_count.sum() + race_cities_df.runner_count.sum())\n",
    "print(f\"Mapped: {race_cities_df.runner_count.sum()}\")\n",
    "print(f\"Unmapped: {missing_cities.runner_count.sum()}\")\n",
    "print(f\"Coverage: {mapped_amount*100:>11.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b45124b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from time import sleep\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "def get_historical_weather(lat, lon, start_date, end_date):\n",
    "    \"\"\"Fetch daily weather from Open-Meteo archive API.\"\"\"\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": start_date,\n",
    "        \"end_date\": end_date,\n",
    "        \"daily\": [\"temperature_2m_max\", \"temperature_2m_min\", \"precipitation_sum\"],\n",
    "        \"temperature_unit\": \"fahrenheit\",\n",
    "        \"timezone\": \"auto\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Weather API Error: {response.status_code}\")\n",
    "        print(f\"Message: {response.text}\")\n",
    "        if response.status_code == 429:\n",
    "            print(\"sleeping for 60 seconds...\")\n",
    "            sleep(60)\n",
    "        raise Exception(\"exception querying weather api\")\n",
    "    \n",
    "def fetch_weather_for_cities(df, start_date, end_date, output_path=\"data/weather_data_v2.csv\", save_every=10):\n",
    "    \"\"\"\n",
    "    Fetch weather for all cities, with progress bar and checkpoint saving.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with city, state, lat, lng columns\n",
    "        start_date: Start date string \"YYYY-MM-DD\"\n",
    "        end_date: End date string \"YYYY-MM-DD\"\n",
    "        output_path: Where to save intermediate/final results\n",
    "        save_every: Save checkpoint every N cities\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # Load existing results if resuming\n",
    "    if output_path.exists():\n",
    "        existing_df = pd.read_csv(output_path)\n",
    "        completed = set(zip(existing_df['city'], existing_df['state']))\n",
    "        weather_records = existing_df.to_dict('records')\n",
    "        print(f\"Resuming: found {len(completed)} cities already processed at {output_path}\")\n",
    "    else:\n",
    "        completed = set()\n",
    "        weather_records = []\n",
    "    \n",
    "    # Filter to only cities we haven't done yet\n",
    "    remaining = df[~df.apply(lambda r: (r['city'], r['state']) in completed, axis=1)]\n",
    "    \n",
    "    if len(remaining) == 0:\n",
    "        print(\"All cities already processed!\")\n",
    "        return pd.read_csv(output_path)\n",
    "    \n",
    "    print(f\"Fetching weather for {len(remaining)} cities...\")\n",
    "    \n",
    "    for i, (idx, row) in enumerate(tqdm(remaining.iterrows(), total=len(remaining), desc=\"Fetching weather\")):\n",
    "        try:\n",
    "            print(f\"Fetching weather for {row[\"city\"]}...\")\n",
    "            data = get_historical_weather(row['lat'], row['lng'], start_date, end_date)\n",
    "            \n",
    "            if data and 'daily' in data:\n",
    "                daily = data['daily']\n",
    "                for j, date in enumerate(daily['time']):\n",
    "                    weather_records.append({\n",
    "                        'city': row['city'],\n",
    "                        'state': row['state'],\n",
    "                        'date': date,\n",
    "                        'temp_max': daily['temperature_2m_max'][j],\n",
    "                        'temp_min': daily['temperature_2m_min'][j],\n",
    "                        'precip': daily['precipitation_sum'][j]\n",
    "                    })\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if (i + 1) % save_every == 0:\n",
    "                print(f\"..incremental save ({len(weather_records)} records) to {output_path}...\")\n",
    "                pd.DataFrame(weather_records).to_csv(output_path, index=False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"Error fetching {row['city']}, {row['state']}: {e}\")\n",
    "            #raise e\n",
    "        \n",
    "        sleep(15)\n",
    "    \n",
    "    # Final save\n",
    "    result_df = pd.DataFrame(weather_records)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved {len(result_df)} records to {output_path}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13b9ed41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": merged.iloc[0].lat,\n",
    "    \"longitude\": merged.iloc[0].lng,\n",
    "    \"start_date\": \"1999-01-01\",\n",
    "    \"end_date\": \"2026-01-01\",\n",
    "    \"daily\": [\"temperature_2m_max\", \"temperature_2m_min\", \"precipitation_sum\"],\n",
    "    \"temperature_unit\": \"fahrenheit\",\n",
    "    \"timezone\": \"auto\"\n",
    "}\n",
    "response = requests.get(url, params=params)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9e18ee1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.37401059859706975)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.runner_count.iloc[0:85].sum()/merged.runner_count.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "zq5lareug6q",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing race cities (excluding 'anywhere'): 886\n",
      "Total results covered: 432,152\n",
      "\n",
      "Top 20 missing cities:\n",
      "             city state  result_count\n",
      "          orlando    fl         32256\n",
      "           duluth    mn         17747\n",
      "     saint george    ut         16667\n",
      "         richmond    va         11455\n",
      "        las vegas    nv         10857\n",
      "carmel-by-the-sea    ca          9459\n",
      "           oracle    az          9213\n",
      "         carlsbad    ca          8415\n",
      "             napa    ca          7888\n",
      " huntington beach    ca          7629\n",
      "      miami beach    fl          6816\n",
      "    newport beach    ca          6487\n",
      "      new orleans    la          6432\n",
      "        baltimore    md          5984\n",
      "           eugene    or          5796\n",
      "     indianapolis    in          5110\n",
      "        vancouver    bc          5020\n",
      "      long branch    nj          5003\n",
      "    kiawah island    sc          4568\n",
      "  manitou springs    co          4084\n"
     ]
    }
   ],
   "source": [
    "# Load missing race cities prioritized by result count\n",
    "missing_race_cities = pd.read_csv('data/missing_race_cities.csv')\n",
    "\n",
    "# Filter out \"anywhere\" entries (virtual/international races without specific location)\n",
    "missing_race_cities = missing_race_cities[missing_race_cities['city'] != 'anywhere']\n",
    "\n",
    "print(f\"Missing race cities (excluding 'anywhere'): {len(missing_race_cities)}\")\n",
    "print(f\"Total results covered: {missing_race_cities['result_count'].sum():,}\")\n",
    "print(f\"\\nTop 20 missing cities:\")\n",
    "print(missing_race_cities.head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "yn4ms9r0wih",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully mapped: 720 / 887 cities\n",
      "Missing coordinates: 167 cities\n",
      "\n",
      "Cities without coordinates:\n",
      "                     city state  result_count\n",
      "             saint george    ut         16667\n",
      "                vancouver    bc          5020\n",
      "white sands missile range    nm          3392\n",
      "              saint louis    mo          2954\n",
      "                 falmouth    ma          1806\n",
      "               bar harbor    me          1732\n",
      "                 victoria    bc          1501\n",
      "                  ventura    ca          1398\n",
      "             east hampton    ny          1314\n",
      "                   tanner    wa          1025\n",
      "      carrabassett valley    me           684\n",
      "                  bristol    nh           578\n",
      "               waitsfield    vt           545\n",
      "            saint charles    il           522\n",
      "               barnstable    ma           506\n",
      "                   sparks    md           496\n",
      "             saint joseph    mn           482\n",
      "                     todd    nc           393\n",
      "             tomkins cove    ny           367\n",
      "            niagara falls    on           350\n",
      "\n",
      "Ready to fetch weather for 720 cities\n",
      "Potential coverage: 385,050 additional race results\n"
     ]
    }
   ],
   "source": [
    "# Map missing cities to lat/lng coordinates\n",
    "missing_mapped = missing_race_cities.merge(\n",
    "    cities_df_p[['city', 'state', 'lat', 'lng']], \n",
    "    on=['city', 'state'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check mapping success\n",
    "has_coords = missing_mapped[['lat', 'lng']].notna().all(axis=1)\n",
    "print(f\"Successfully mapped: {has_coords.sum()} / {len(missing_mapped)} cities\")\n",
    "print(f\"Missing coordinates: {(~has_coords).sum()} cities\")\n",
    "\n",
    "if (~has_coords).sum() > 0:\n",
    "    print(\"\\nCities without coordinates:\")\n",
    "    print(missing_mapped[~has_coords][['city', 'state', 'result_count']].head(20).to_string(index=False))\n",
    "\n",
    "# Keep only cities with coordinates\n",
    "missing_mapped_valid = missing_mapped[has_coords].copy()\n",
    "missing_mapped_valid = missing_mapped_valid.sort_values('result_count', ascending=False)\n",
    "\n",
    "print(f\"\\nReady to fetch weather for {len(missing_mapped_valid)} cities\")\n",
    "print(f\"Potential coverage: {missing_mapped_valid['result_count'].sum():,} additional race results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3w01swdeaw2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_incremental(cities_df, start_date, end_date, output_path=\"data/weather_data_v2.csv\", \n",
    "                              batch_size=10, delay_between=15):\n",
    "    \"\"\"\n",
    "    Fetch weather incrementally with better progress tracking and API limit handling.\n",
    "    \n",
    "    Args:\n",
    "        cities_df: DataFrame with city, state, lat, lng, result_count columns\n",
    "        start_date: Start date \"YYYY-MM-DD\"\n",
    "        end_date: End date \"YYYY-MM-DD\"\n",
    "        output_path: Output CSV path\n",
    "        batch_size: Number of cities to fetch before reporting progress\n",
    "        delay_between: Seconds to wait between API calls (default 15 for free tier)\n",
    "    \"\"\"\n",
    "    output_path = Path(output_path)\n",
    "    \n",
    "    # Load existing data\n",
    "    if output_path.exists():\n",
    "        existing_df = pd.read_csv(output_path)\n",
    "        completed = set(zip(existing_df['city'], existing_df['state']))\n",
    "        weather_records = existing_df.to_dict('records')\n",
    "        print(f\"Found {len(completed)} cities already in {output_path}\")\n",
    "    else:\n",
    "        completed = set()\n",
    "        weather_records = []\n",
    "        print(f\"Starting fresh - no existing data at {output_path}\")\n",
    "    \n",
    "    # Filter to cities we haven't done\n",
    "    remaining = cities_df[~cities_df.apply(lambda r: (r['city'], r['state']) in completed, axis=1)].copy()\n",
    "    \n",
    "    if len(remaining) == 0:\n",
    "        print(\"All cities already processed!\")\n",
    "        return pd.read_csv(output_path)\n",
    "    \n",
    "    print(f\"\\nRemaining cities to fetch: {len(remaining)}\")\n",
    "    print(f\"Estimated time: ~{len(remaining) * delay_between / 60:.1f} minutes\")\n",
    "    print(f\"Potential new results covered: {remaining['result_count'].sum():,}\")\n",
    "    \n",
    "    # Show progress tracking\n",
    "    total_new_records_target = len(remaining) * ((pd.to_datetime(end_date) - pd.to_datetime(start_date)).days + 1)\n",
    "    print(f\"Expected weather records: ~{total_new_records_target:,}\")\n",
    "    \n",
    "    successful = 0\n",
    "    failed = 0\n",
    "    \n",
    "    for i, (idx, row) in enumerate(remaining.iterrows(), 1):\n",
    "        try:\n",
    "            print(f\"[{i}/{len(remaining)}] {row['city']}, {row['state']} ({row['result_count']:,} results)...\", \n",
    "                  end=' ', flush=True)\n",
    "            \n",
    "            data = get_historical_weather(row['lat'], row['lng'], start_date, end_date)\n",
    "            \n",
    "            if data and 'daily' in data:\n",
    "                daily = data['daily']\n",
    "                records_added = 0\n",
    "                for j, date in enumerate(daily['time']):\n",
    "                    weather_records.append({\n",
    "                        'city': row['city'],\n",
    "                        'state': row['state'],\n",
    "                        'date': date,\n",
    "                        'temp_max': daily['temperature_2m_max'][j],\n",
    "                        'temp_min': daily['temperature_2m_min'][j],\n",
    "                        'precip': daily['precipitation_sum'][j]\n",
    "                    })\n",
    "                    records_added += 1\n",
    "                \n",
    "                print(f\"‚úì {records_added} records\")\n",
    "                successful += 1\n",
    "                \n",
    "                # Save progress every batch_size cities\n",
    "                if i % batch_size == 0:\n",
    "                    pd.DataFrame(weather_records).to_csv(output_path, index=False)\n",
    "                    current_coverage = (85 + successful) / 977 * 100  # 977 total race cities\n",
    "                    print(f\"\\n  ‚Üí Checkpoint: {successful} cities fetched, {len(weather_records):,} total records\")\n",
    "                    print(f\"  ‚Üí Estimated coverage: ~{current_coverage:.1f}%\\n\")\n",
    "            else:\n",
    "                print(\"‚úó No data returned\")\n",
    "                failed += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "            failed += 1\n",
    "            \n",
    "            # If we hit rate limits, save progress and stop\n",
    "            if \"429\" in str(e) or \"limit\" in str(e).lower():\n",
    "                print(f\"\\n‚ö† API rate limit reached after {successful} successful fetches\")\n",
    "                print(f\"Saving progress and stopping...\")\n",
    "                break\n",
    "        \n",
    "        # Wait between requests to respect API limits\n",
    "        if i < len(remaining):  # Don't wait after last city\n",
    "            sleep(delay_between)\n",
    "    \n",
    "    # Final save\n",
    "    result_df = pd.DataFrame(weather_records)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Completed: {successful} successful, {failed} failed\")\n",
    "    print(f\"Total weather records: {len(result_df):,}\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "wu1oxjfndcr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Fetching weather for top 25 missing cities\n",
      "================================================================================\n",
      "Expected coverage increase: ~15% (64.7% ‚Üí ~79.8%)\n",
      "Expected time: ~6.2 minutes at 15 seconds per city\n",
      "Results covered: 191,661\n",
      "\n",
      "Cities to fetch:\n",
      "             city state  result_count\n",
      "          orlando    fl         32256\n",
      "           duluth    mn         17747\n",
      "         richmond    va         11455\n",
      "        las vegas    nv         10857\n",
      "carmel-by-the-sea    ca          9459\n",
      "           oracle    az          9213\n",
      "         carlsbad    ca          8415\n",
      "             napa    ca          7888\n",
      " huntington beach    ca          7629\n",
      "      miami beach    fl          6816\n",
      "... and 15 more\n",
      "Found 85 cities already in data/weather_data_v2.csv\n",
      "\n",
      "Remaining cities to fetch: 25\n",
      "Estimated time: ~6.2 minutes\n",
      "Potential new results covered: 191,661\n",
      "Expected weather records: ~246,575\n",
      "[1/25] orlando, fl (32,256 results)... ‚úì 9863 records\n",
      "[2/25] duluth, mn (17,747 results)... ‚úì 9863 records\n",
      "[3/25] richmond, va (11,455 results)... ‚úì 9863 records\n",
      "[4/25] las vegas, nv (10,857 results)... ‚úì 9863 records\n",
      "[5/25] carmel-by-the-sea, ca (9,459 results)... ‚úì 9863 records\n",
      "[6/25] oracle, az (9,213 results)... Weather API Error: 429\n",
      "Message: {\"error\":true,\"reason\":\"Minutely API request limit exceeded. Please try again in one minute.\"}\n",
      "sleeping for 60 seconds...\n",
      "‚úó Error: exception querying weather api\n",
      "[7/25] carlsbad, ca (8,415 results)... ‚úì 9863 records\n",
      "[8/25] napa, ca (7,888 results)... ‚úì 9863 records\n",
      "[9/25] huntington beach, ca (7,629 results)... ‚úì 9863 records\n",
      "[10/25] miami beach, fl (6,816 results)... ‚úì 9863 records\n",
      "\n",
      "  ‚Üí Checkpoint: 9 cities fetched, 910,265 total records\n",
      "  ‚Üí Estimated coverage: ~9.6%\n",
      "\n",
      "[11/25] newport beach, ca (6,487 results)... ‚úì 9863 records\n",
      "[12/25] new orleans, la (6,432 results)... ‚úì 9863 records\n",
      "[13/25] baltimore, md (5,984 results)... ‚úì 9863 records\n",
      "[14/25] eugene, or (5,796 results)... ‚úì 9863 records\n",
      "[15/25] indianapolis, in (5,110 results)... ‚úì 9863 records\n",
      "[16/25] long branch, nj (5,003 results)... ‚úì 9863 records\n",
      "[17/25] kiawah island, sc (4,568 results)... ‚úì 9863 records\n",
      "[18/25] manitou springs, co (4,084 results)... ‚úì 9863 records\n",
      "[19/25] burlington, vt (4,072 results)... ‚úì 9863 records\n",
      "[20/25] ojai, ca (3,950 results)... ‚úì 9863 records\n",
      "\n",
      "  ‚Üí Checkpoint: 19 cities fetched, 1,008,895 total records\n",
      "  ‚Üí Estimated coverage: ~10.6%\n",
      "\n",
      "[21/25] lincoln, ne (3,896 results)... ‚úì 9863 records\n",
      "[22/25] logan, ut (3,846 results)... ‚úì 9863 records\n",
      "[23/25] myrtle beach, sc (3,735 results)... ‚úì 9863 records\n",
      "[24/25] cleveland, oh (3,485 results)... ‚úì 9863 records\n",
      "[25/25] huntsville, al (3,478 results)... ‚úì 9863 records\n",
      "\n",
      "================================================================================\n",
      "Completed: 24 successful, 1 failed\n",
      "Total weather records: 1,058,210\n",
      "Saved to: data/weather_data_v2.csv\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Fetch weather for top N missing cities\n",
    "# Start with top 25 to get from 64.7% to ~79.8% coverage\n",
    "\n",
    "# Select top cities to fetch\n",
    "TOP_N = 25  # Adjust this number based on API limits and time available\n",
    "cities_to_fetch = missing_mapped_valid.head(TOP_N)\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Fetching weather for top {TOP_N} missing cities\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Expected coverage increase: ~15% (64.7% ‚Üí ~79.8%)\")\n",
    "print(f\"Expected time: ~{TOP_N * 15 / 60:.1f} minutes at 15 seconds per city\")\n",
    "print(f\"Results covered: {cities_to_fetch['result_count'].sum():,}\")\n",
    "print(f\"\\nCities to fetch:\")\n",
    "print(cities_to_fetch[['city', 'state', 'result_count']].head(10).to_string(index=False))\n",
    "if TOP_N > 10:\n",
    "    print(f\"... and {TOP_N - 10} more\")\n",
    "\n",
    "# Note: Uncomment the line below to actually start fetching\n",
    "weather_df = fetch_weather_incremental(cities_to_fetch, \"1999-01-01\", \"2026-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aodyxm597b",
   "metadata": {},
   "source": [
    "## Weather Data Expansion Strategy\n",
    "\n",
    "### Current Status\n",
    "- **85 cities** with weather data ‚Üí **64.7% coverage**\n",
    "- **912 missing cities** (excluding \"anywhere\" virtual races)\n",
    "\n",
    "### Incremental Coverage Plan\n",
    "\n",
    "| Cities Added | Coverage | Improvement | Est. Time (15s/city) |\n",
    "|--------------|----------|-------------|----------------------|\n",
    "| Top 10       | 74.7%    | +9.9%       | ~2.5 minutes         |\n",
    "| Top 25       | 79.8%    | +15.1%      | ~6 minutes           |\n",
    "| Top 50       | 84.4%    | +19.7%      | ~12 minutes          |\n",
    "| Top 100      | 88.5%    | +23.8%      | ~25 minutes          |\n",
    "| Top 200      | 91.9%    | +27.2%      | ~50 minutes          |\n",
    "\n",
    "### API Options\n",
    "\n",
    "**Open-Meteo Free Tier** (currently using)\n",
    "- ‚úÖ Free, no API key needed\n",
    "- ‚úÖ Historical data from 1940-present\n",
    "- ‚ùå Rate limit: ~10,000 requests/day\n",
    "- üìä Good for: Adding 10-50 cities per day\n",
    "\n",
    "**Alternative APIs** (if needed)\n",
    "1. **Visual Crossing** - Free tier: 1000 records/day\n",
    "2. **WeatherAPI.com** - Free tier: 1M calls/month  \n",
    "3. **NOAA API** - Free, unlimited (US only, requires station mapping)\n",
    "\n",
    "### Recommended Approach\n",
    "1. Start with top 25 cities (~15% improvement)\n",
    "2. Run daily batches to avoid rate limits\n",
    "3. Monitor API responses and save progress checkpoints\n",
    "4. Consider paid tier ($40-50 one-time) if faster completion needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4ydw5vn2h77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running enrichment script to update race day weather...\n",
      "This will update: data/featurized_race_data_v2_with_raceday_weather.csv\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Race Day Weather Enrichment\n",
      "================================================================================\n",
      "\n",
      "Loading data files...\n",
      "================================================================================\n",
      "1. Loading featurized_race_data_v2.csv... ‚úì 1,688,270 records\n",
      "2. Loading race_locations_normalized.csv... ‚úì 1,857 races\n",
      "3. Loading weather_data_v2.csv... ‚úì 1,058,210 daily records\n",
      "\n",
      "================================================================================\n",
      "Step 1: Joining with race locations...\n",
      "================================================================================\n",
      "Records with race location: 1,595,734 (94.5%)\n",
      "Records without race location: 92,536 (5.5%)\n",
      "\n",
      "Unique races without location: 67\n",
      "First 10 unmapped races:\n",
      "  - 26.2_with_donna:_the_national_marathon_to_finish_breast_cancer\n",
      "  - alaska_series_-_day_1\n",
      "  - alaska_series_-_day_2\n",
      "  - alaska_series_-_day_3\n",
      "  - alaska_series_-_day_4\n",
      "  - appalachian_series_-_al\n",
      "  - appalachian_series_-_ga\n",
      "  - appalachian_series_-_nc\n",
      "  - appalachian_series_-_sc\n",
      "  - appalachian_series_-_tn\n",
      "\n",
      "================================================================================\n",
      "Step 2: Joining with race day weather...\n",
      "================================================================================\n",
      "Records with race day weather: 1,275,345 (75.5%)\n",
      "Records without race day weather: 412,925 (24.5%)\n",
      "\n",
      "Breakdown of missing weather:\n",
      "  - No race location: 92,536\n",
      "  - Has location but no weather: 320,389\n",
      "\n",
      "Sample races with location but no weather (likely international):\n",
      "  - \"last_chance_for_boston\"_marathon on 2003-02-02 in dublin, oh\n",
      "  - \"last_chance_for_boston\"_marathon on 2004-02-01 in dublin, oh\n",
      "  - \"last_chance_for_boston\"_marathon on 2005-02-20 in dublin, oh\n",
      "  - \"last_chance_for_boston\"_marathon on 2007-02-11 in dublin, oh\n",
      "  - \"last_chance_for_boston\"_marathon on 2010-02-28 in dublin, oh\n",
      "  - \"last_chance_for_boston\"_marathon on 2011-02-27 in dublin, oh\n",
      "  - \"last_chance_for_boston\"_marathon on 2012-02-26 in dublin, oh\n",
      "  - \"last_chance_for_boston\"_marathon on 2016-02-21 in dublin, oh\n",
      "  - \"running_for_the_bay!\"_marathon on 2010-10-23 in apalachicola, fl\n",
      "  - \"running_for_the_bay!\"_marathon on 2011-10-23 in apalachicola, fl\n",
      "\n",
      "================================================================================\n",
      "Data Validation:\n",
      "================================================================================\n",
      "‚úì Record count preserved: 1,688,270\n",
      "‚úì Column 'race_location_city' added\n",
      "‚úì Column 'race_location_state' added\n",
      "‚úì Column 'race_day_temp_min' added\n",
      "‚úì Column 'race_day_temp_max' added\n",
      "‚úì Column 'race_day_precip' added\n",
      "\n",
      "Weather value ranges:\n",
      "  Temperature min: -2.0¬∞F to 90.1¬∞F\n",
      "  Temperature max: 19.2¬∞F to 109.8¬∞F\n",
      "  Precipitation: 0.0 to 76.7 inches\n",
      "    ‚ö† WARNING: Unusual precipitation range!\n",
      "\n",
      "================================================================================\n",
      "Sample of Enriched Data:\n",
      "================================================================================\n",
      "\n",
      "Boston Marathon sample (first 5 records):\n",
      "                             race       date race_location_city race_location_state  race_day_temp_min  race_day_temp_max  race_day_precip       city state  age sex       time\n",
      "\"last_chance_for_boston\"_marathon 2003-02-02             dublin                  oh                NaN                NaN              NaN  ann arbor    mi 41.0   M 203.450000\n",
      "\"last_chance_for_boston\"_marathon 2003-02-02             dublin                  oh                NaN                NaN              NaN  champaign    il 48.0   M 203.366667\n",
      "\"last_chance_for_boston\"_marathon 2003-02-02             dublin                  oh                NaN                NaN              NaN    chicago    il 26.0   F 220.983333\n",
      "\"last_chance_for_boston\"_marathon 2003-02-02             dublin                  oh                NaN                NaN              NaN cincinnati    oh 43.0   M 194.733333\n",
      "\"last_chance_for_boston\"_marathon 2003-02-02             dublin                  oh                NaN                NaN              NaN cincinnati    oh 31.0   M 213.250000\n",
      "\n",
      "================================================================================\n",
      "Saving enriched data...\n",
      "================================================================================\n",
      "‚úì Saved to: data/featurized_race_data_v2_with_raceday_weather.csv\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "================================================================================\n",
      "Total records: 1,688,270\n",
      "Records with complete enrichment: 1,275,345 (75.5%)\n",
      "New columns added: 5\n",
      "  - race_location_city\n",
      "  - race_location_state\n",
      "  - race_day_temp_min\n",
      "  - race_day_temp_max\n",
      "  - race_day_precip\n",
      "\n",
      "================================================================================\n",
      "Enrichment complete!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Enrichment complete! Check the coverage statistics above.\n"
     ]
    }
   ],
   "source": [
    "# After fetching more weather data, re-run the enrichment script to update coverage\n",
    "# This will merge the new weather data with the race results\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def update_race_weather_enrichment():\n",
    "    \"\"\"\n",
    "    Re-run the enrichment script to update race day weather with newly added cities.\n",
    "    \"\"\"\n",
    "    print(\"Running enrichment script to update race day weather...\")\n",
    "    print(\"This will update: data/featurized_race_data_v2_with_raceday_weather.csv\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['python', 'enrich_race_day_weather.py'],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Errors:\", result.stderr)\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"Enrichment complete! Check the coverage statistics above.\")\n",
    "    \n",
    "# Uncomment to re-run enrichment after adding weather data:\n",
    "update_race_weather_enrichment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
